{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <center>Hold 05 - Project group 22 </center> </h3>\n",
    "<center>Rune Ejnar Bang Lejbølle (nvr889@alumni.ku.dk)</center>\n",
    "<center>Pedram Bakhtiarifard (lcd842@alumni.ku.dk)</center>\n",
    "\n",
    "<h1><center>Milestone 2</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <center> Loading the data </center> </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas.io.sql as psql\n",
    "\n",
    "# Load from local CSV-file\n",
    "\n",
    "nrows = 100000\n",
    "\n",
    "source_data = pd.read_csv(\"500thousand_rows.csv\", index_col=None, nrows=nrows, dtype={\"id\": \"string\", \"domain\": \"string\", \"type\": \"string\", \"url\": \"string\", \"content\": \"string\", \"scraped_at\": \"string\", \"inserted_at\": \"string\", \"updated_at\": \"string\", \"title\": \"string\", \"authors\": \"string\", \"keywords\": \"string\", \"meta_keywords\": \"string\", \"meta_description\": \"string\", \"tags\": \"string\", \"summary\": \"string\", \"source\": \"string\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from cleantext import constants as cleantext_re\n",
    "\n",
    "punctuation_pattern = re.compile(r'([!\"#$%&\\'’()*+,\\-–—./:;=?@\\[\\\\\\]^_`{}~<>\\n\\t\\r])')\n",
    "\n",
    "source_data['cleaned_content'] = source_data['content']\\\n",
    "    .str.replace(cleantext_re.EMAIL_REGEX, '|email|')\\\n",
    "    .str.replace(cleantext_re.URL_REGEX, '|url|')\\\n",
    "    .str.replace(cleantext_re.NUMBERS_REGEX, '|num|')\\\n",
    "    .str.replace(punctuation_pattern, \"\")\\\n",
    "    .str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ped/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "source_data = source_data.dropna(subset=['cleaned_content'])\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r'(\\w+|\\|\\w+\\|)')\n",
    "\n",
    "source_data['tokenized_content'] = source_data['cleaned_content'].apply(tokenizer.tokenize)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    return [x for x in tokens if not x in stopwords]\n",
    "\n",
    "\n",
    "source_data['nostop_content'] = source_data['tokenized_content'].apply(filter_stopwords)\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    input = source_data['nostop_content'].to_numpy()\n",
    "    output = numpy.empty_like(input)\n",
    "\n",
    "    results = executor.map(stem_tokens, input)\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        output[i] = result\n",
    "\n",
    "    source_data['stemmed_content'] = output\n",
    "\n",
    "source_data['joined_content'] = source_data['stemmed_content'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "array_pattern = re.compile(r'[^, \\[\\]\\']+(?: [^, \\[\\]\\']+)*')\n",
    "list_pattern = re.compile(r'[^, ]+(?: [^, ]+)*')\n",
    "\n",
    "def make_list_atomic(source_data, key, to_split, pattern):\n",
    "    source_data[to_split + '_split'] = source_data[to_split].str.lower().str.findall(pattern)\n",
    "    df = source_data[[key, to_split + '_split']][source_data[to_split + '_split'].notnull()]\\\n",
    "        .explode(to_split + '_split', ignore_index=True)\n",
    "    return df[df[to_split + '_split'].notnull()]\n",
    "\n",
    "database_authors = make_list_atomic(source_data, 'id', 'authors', list_pattern).drop_duplicates()\n",
    "database_keywords = make_list_atomic(source_data, 'id', 'keywords', list_pattern).drop_duplicates()\n",
    "database_meta_keywords = make_list_atomic(source_data, 'id', 'meta_keywords', array_pattern).drop_duplicates()\n",
    "database_tags = make_list_atomic(source_data, 'id', 'tags', list_pattern).drop_duplicates()\n",
    "\n",
    "database_articles = source_data[['id', 'domain', 'type', 'url', 'joined_content',\n",
    "                                 'scraped_at', 'inserted_at', 'updated_at', 'title', 'meta_description', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn_string = 'postgres://postgres@localhost/fakenews_data'\n",
    "\n",
    "pg_conn = psycopg2.connect(conn_string)\n",
    "\n",
    "cur = pg_conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "table_create_sql = '''\n",
    "CREATE TABLE IF NOT EXISTS Articles (\n",
    "    id INT PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    url TEXT,\n",
    "    domain TEXT,\n",
    "    type TEXT,\n",
    "    cleaned_content TEXT,\n",
    "    scraped_at TIMESTAMP,\n",
    "    inserted_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    meta_description TEXT,\n",
    "    summary TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS Authors (\n",
    "    author_name TEXT,\n",
    "    article_id INT,\n",
    "    PRIMARY KEY (author_name, article_id),\n",
    "    FOREIGN KEY (article_id) REFERENCES Articles(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS Keywords (\n",
    "    keyword_name TEXT,\n",
    "    article_id INT,\n",
    "    PRIMARY KEY (keyword_name, article_id),\n",
    "    FOREIGN KEY (article_id) REFERENCES Articles(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS Meta_Keywords (\n",
    "    meta_keyword_name TEXT,\n",
    "    article_id INT,\n",
    "    PRIMARY KEY (meta_keyword_name, article_id),\n",
    "    FOREIGN KEY (article_id) REFERENCES Articles(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS Tags (\n",
    "    tag_name TEXT,\n",
    "    article_id INT,\n",
    "    PRIMARY KEY (tag_name, article_id),\n",
    "    FOREIGN KEY (article_id) REFERENCES Articles(id)\n",
    ");\n",
    "'''\n",
    "\n",
    "cur.execute(table_create_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trigger_create_sql = '''\n",
    "CREATE OR REPLACE FUNCTION pass() RETURNS trigger AS $$\n",
    "    BEGIN\n",
    "        RETURN NULL;\n",
    "    END;\n",
    "$$ LANGUAGE plpgsql ;\n",
    "\n",
    "CREATE TRIGGER InsertedNotNull\n",
    "    BEFORE INSERT ON Authors\n",
    "    FOR EACH ROW\n",
    "    WHEN ( NEW.author_name IS NULL )\n",
    "    EXECUTE PROCEDURE pass();\n",
    "\n",
    "CREATE TRIGGER InsertedNotNull\n",
    "    BEFORE INSERT ON Keywords\n",
    "    FOR EACH ROW\n",
    "    WHEN ( NEW.keyword_name IS NULL )\n",
    "    EXECUTE PROCEDURE pass();\n",
    "\n",
    "CREATE TRIGGER InsertedNotNull\n",
    "    BEFORE INSERT ON Meta_Keywords\n",
    "    FOR EACH ROW\n",
    "    WHEN ( NEW.meta_keyword_name IS NULL )\n",
    "    EXECUTE PROCEDURE pass();\n",
    "\n",
    "CREATE TRIGGER InsertedNotNull\n",
    "    BEFORE INSERT ON Tags\n",
    "    FOR EACH ROW\n",
    "    WHEN ( NEW.tag_name IS NULL )\n",
    "    EXECUTE PROCEDURE pass();\n",
    "'''\n",
    "\n",
    "cur.execute(trigger_create_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "postgres_data_path = \"/postgres_data/\"\n",
    "\n",
    "database_articles.to_csv(postgres_data_path + 'database_articles.csv', index=False)\n",
    "database_authors.to_csv(postgres_data_path + 'database_authors.csv', index=False)\n",
    "database_keywords.to_csv(postgres_data_path + 'database_keywords.csv', index=False)\n",
    "database_meta_keywords.to_csv(postgres_data_path + 'database_meta_keywords.csv', index=False)\n",
    "database_tags.to_csv(postgres_data_path + 'database_tags.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "copy_sql = \"\"\"\n",
    "COPY Articles(id, domain, type, url, cleaned_content, scraped_at, inserted_at, updated_at, title, meta_description, summary)\n",
    "FROM '\"\"\" + postgres_data_path + \"\"\"database_articles.csv'\n",
    "DELIMITER ','\n",
    "CSV HEADER;\n",
    "\n",
    "COPY Authors(article_id, author_name) FROM '\"\"\" + postgres_data_path + \"\"\"database_authors.csv'\n",
    "DELIMITER ','\n",
    "CSV HEADER ;\n",
    "\n",
    "COPY Keywords(article_id, keyword_name) FROM '\"\"\" + postgres_data_path + \"\"\"database_keywords.csv'\n",
    "DELIMITER ','\n",
    "CSV HEADER ;\n",
    "\n",
    "COPY Meta_Keywords(article_id, meta_keyword_name) FROM '\"\"\" + postgres_data_path + \"\"\"database_meta_keywords.csv'\n",
    "DELIMITER ','\n",
    "CSV HEADER ;\n",
    "\n",
    "COPY Tags(article_id, tag_name) FROM '\"\"\" + postgres_data_path + \"\"\"database_tags.csv'\n",
    "DELIMITER ','\n",
    "CSV HEADER ;\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(copy_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pg_conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "The database schemas are designed with the objective of reducing redundancy and accomidating future extensions of the database. In particular, we note that the articles of FakeNewsCorpus dataset in some cases have several authors. If authors were to part of a articles schema, there would be significant redundency in the database, considering that tuples would exist with identicial information for all but the author attribute. Therefore, a seperate schema for authors have been created, additionaly making all attributes atomic data types, which helps avoid repeating groups and consequently, in some scenarios, inefficient queries.\n",
    "Similar arguments holds for keywords, meta keywords and tags for articles in the dataset, hence, the choice of seperating them into their own database schemas consiting of atomic attributes only. In turn, this aspect of database normilization allows the database to be extended upon without neccesarily changing the exisiting structure.\n",
    "\n",
    "One downside of our choice of realations is that we in some cases will have several tuples cointaing information about the same entity. Using the Authors relation as an example, each author will have a tuple for each article they have authored. If no author names are the same, and names are always the same, this is not a problem. However, if this is not the case, it could cause several problems with maintaining information about authors.\n",
    "\n",
    "The first simple problem, is that of update anomalies. If an author changes name, we must make sure to change his name in all tuples containing his name, so it is clear these articles still have the same author.\n",
    "\n",
    "A more complicated issue is the problem of entity resolution. Using this design it can be hard to tell which articles actually have the same author. Some authors could have the same name, and some names could be spelled diferently in diferent articles, even though the author was the same.\n",
    "\n",
    "One way to combat this issue would be to split the relation into two relations. One relation containing author names, and some form of unique id, and another containing tuples of author ids and article ids showing the authors of each article. However, such a database would require more data, computation and space to maintain, and we therefore choose to go with the simpler design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    count\n",
      "0  100000\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT count(*) FROM articles;\n",
    "'''\n",
    "\n",
    "print(psql.read_sql(query, pg_conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 2\n",
    "\n",
    "We created the following relations for our database:\n",
    "\n",
    "* Articles\n",
    "* Authors\n",
    "* Keywords\n",
    "* Meta_Keywords\n",
    "* Tags\n",
    "\n",
    "The Articles relation contains the main information about the articles, and has the attributes \"id\", \"domain\", \"type\", \"url\", \"content\", \"scraped_at\", \"inserted_at\", \"updated_at\", \"title\", \"meta_description\", \"summary\".\n",
    "The single primary key for this relation is the \"id\", which therefore functionally determines all other attributes.\n",
    "\n",
    "The Authors relation contains tuples of authors and articles wrtitten by these authors, referenced by the \"id\" attribute of the articles. The relation therefore has the attributes \"article_id\" and \"author_name\", the combination of which form the primary key. There are therefore no nontrivial functoinal dependencies.\n",
    "\n",
    "The Keywords relation contains the attributes \"article_id\" and \"keyword_name\". As with the Authors relation, both attributes together form the primary key for the relation, and there are therefore no nontrivial functional dependencies.\n",
    "\n",
    "The Meta_Keywords relation has similar structure, and contains the attributes \"article_id\" and \"meta_keyword_name\". Since both attributes form the primary key, there are no nontrivial functional dependencies.\n",
    "\n",
    "Finally, the Tags relation also has a similar structure, with the attributes and also the primary key being \"article_id\" and \"tag_name\", resulting in no nontrivial functional dependencies.\n",
    "\n",
    "All relations therefore respect the BCNF condiction and are in BCNF form.\n",
    "\n",
    "Looking at the data, it may seem like domains uniquly determine the type of article, which would violate the BCNF condition. We, however, would argue that this is not neccesarily the case for all data that could be inserted in the database, as we believe there could be sites producing both real and fake news. We have therefore choosen to stick with this database design, even though it causes some redundancy with the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. List the domains of news articles of reliable type and scraped at or after January 15, 2018. NOTE: Do not include duplicate domains in your answer. [Languages: relational algebra and SQL]\n",
    "\n",
    "**Relational Algebra:**\n",
    "\n",
    "$$ \\pi_{domain}\\left(\\sigma_{type ~=~ 'reliable' ~\\land~ scraped\\_at ~\\geq~ '2018-01-15 00:00:00.0'}(R_{articles})\\right) $$\n",
    "\n",
    "**SQL:\n",
    "**\n",
    "```sql\n",
    "SELECT DISTINCT domain FROM articles\n",
    "WHERE type = 'reliable' AND scraped_at >= '2018-01-15 00:00:00.0';\n",
    "```\n",
    "\n",
    "**Observation(s):**\n",
    "Executing the query on our database yields the following domains:\n",
    "\n",
    "`christianpost.com, consortiumnews.com`\n",
    "\n",
    "This shows that only two news sources have produced articles of *reliable* type at or before January 15, 2018. Considering that there are a total of 246 different domains in our dataset, it is a low proportion of news sources, at least in that time frame, producing articles that are *reliable*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               domain\n",
      "0   christianpost.com\n",
      "1  consortiumnews.com\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT DISTINCT domain FROM articles\n",
    "WHERE type = 'reliable' AND scraped_at >= '2018-01-15 00:00:00.0';\n",
    "'''\n",
    "\n",
    "print(psql.read_sql(query, pg_conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. List the name(s) of the most prolific author(s) of news articles of fake type. An author is among the most prolific if it has authored as many or more fake news articles as any other author in the dataset. [Languages: extended relational algebra and SQL]\n",
    "\n",
    "**Relational Algebra:**\n",
    "\n",
    "$$ P := \\gamma_{author\\_name, COUNT(id)}(R_{articles} \\bowtie R_{authors}) $$\n",
    "$$ Q := \\pi_{MAX(count)}(P) $$\n",
    "$$ \\pi_{author\\_name}\\left(\\sigma_{count ~\\in~ Q}(P)\\right) $$\n",
    "\n",
    "**SQL:**\n",
    "\n",
    "```sql\n",
    "WITH P as (\n",
    "    SELECT a.author_name, count(*) count FROM Articles\n",
    "    JOIN authors a on articles.id = a.article_id\n",
    "    WHERE type = 'fake'\n",
    "    GROUP BY a.author_name\n",
    ")\n",
    "\n",
    "SELECT author_name, count FROM P\n",
    "WHERE count IN (\n",
    "    SELECT max(count) from P\n",
    ");\n",
    "```\n",
    "\n",
    "Observation(s):\n",
    "Executing the query on our database tells us that the author John Rolls has created the most fake news articles. In fact the database contains a total of 876 fake news articles from him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  author_name  count\n",
      "0  john rolls    876\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "WITH P as (\n",
    "    SELECT a.author_name, count(*) count FROM Articles\n",
    "    JOIN authors a on articles.id = a.article_id\n",
    "    WHERE type = 'fake'\n",
    "    GROUP BY a.author_name\n",
    ")\n",
    "\n",
    "SELECT author_name, count FROM P\n",
    "WHERE count IN (\n",
    "    SELECT max(count) from P\n",
    ");\n",
    "'''\n",
    "\n",
    "print(psql.read_sql(query, pg_conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Count the pairs of article IDs that exhibit the exact same set of meta-keywords, but only return the pairs where the set of meta-keywords is not empty. [Language: SQL]\n",
    "\n",
    "SQL:\n",
    "\n",
    "```sql\n",
    "SELECT count(*) FROM articles a1\n",
    "CROSS JOIN articles a2\n",
    "WHERE a1.id < a2.id\n",
    "AND EXISTS(\n",
    "    SELECT mk1.meta_keyword_name FROM meta_keywords mk1\n",
    "    WHERE mk1.article_id = a1.id\n",
    ")\n",
    "AND NOT EXISTS(\n",
    "    SELECT mk1.meta_keyword_name FROM meta_keywords mk1\n",
    "    WHERE mk1.article_id = a1.id\n",
    "    EXCEPT (\n",
    "        SELECT mk2.meta_keyword_name FROM meta_keywords mk2\n",
    "        WHERE mk2.article_id = a2.id\n",
    "    )\n",
    ");\n",
    "```\n",
    "\n",
    "Observation(s):\n",
    "We were not able to finish this query, likely due to the astronomical number of pairs that resulted from the join ($\\binom{100000}{2} \\approx 5 * 10^{11}$). When running on a smaller subset of data (first 1000 rows), there were 28 pairs of articles that matched the criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT count(*) FROM articles a1\n",
    "CROSS JOIN articles a2\n",
    "WHERE a1.id < a2.id\n",
    "AND EXISTS(\n",
    "    SELECT mk1.meta_keyword_name FROM meta_keywords mk1\n",
    "    WHERE mk1.article_id = a1.id\n",
    ")\n",
    "AND NOT EXISTS(\n",
    "    SELECT mk1.meta_keyword_name FROM meta_keywords mk1\n",
    "    WHERE mk1.article_id = a1.id\n",
    "    EXCEPT (\n",
    "        SELECT mk2.meta_keyword_name FROM meta_keywords mk2\n",
    "        WHERE mk2.article_id = a2.id\n",
    "    )\n",
    ");\n",
    "'''\n",
    "\n",
    "#print(psql.read_sql(query1, pg_conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting, almost neccesary query, is that of the proportion of articles of the various types, which the following queuery can answer.\n",
    "\n",
    "From this queuery we see that the dataset contains a disproportionate amount of articles of *fake* type - approximately $46\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          type  count\n",
      "0         bias   9123\n",
      "1    clickbait   1979\n",
      "2   conspiracy   6877\n",
      "3         fake  45768\n",
      "4         hate    298\n",
      "5      junksci   2204\n",
      "6    political  27371\n",
      "7     reliable    289\n",
      "8        rumor    124\n",
      "9       satire    344\n",
      "10     unknown   1067\n",
      "11  unreliable   1045\n",
      "12        None   3511\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT type, count(*) FROM articles\n",
    "GROUP BY type;\n",
    "'''\n",
    "\n",
    "print(psql.read_sql(query, pg_conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further analysis, in respect to fake news detection using these exact articles, it would important to know the volume of data used to learn characteristics of the data, and,perhaps, also the number of unique keywords, meta keywords and tags describing different article types. This is explored executing the following queries.\n",
    "\n",
    "Firstly, we note that the database does not contain any keywords and that not all articles type has meta keywords describing them (e.g. *hate*). The queries with respect to meta keywords tells us that articles of type *reliable* and *fake* has significantly less distinct meta keywords describing them, than for instances, articles of type *conspiracy* and *political*. Out of a total of $29418$ distinct meta keywords, 16542 meta keywords, corresponding to roughly $56\\%$, are used to represent articles of political type and only $\\approx .04$ and $\\approx 0.7\\%$ for articles of type *reliable* and *fake*, respectively.\n",
    "\n",
    "In regards to the set of tags describing different article types, it is seen that article types *rumor*, *hate* and *reliable* has $0.11\\%$, $0.18\\%$ and $0.32\\%$ of the total $36876$ distinct tags, respectively. Analogus to meta keywords, articles of *political* type has highest proportion of tags describing it, that is here roughly $33\\%$.\n",
    "\n",
    "Further interesing and supporting observations could be that of the average amount of meta keywords and tags describing the different article types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORDS:\n",
      "   count\n",
      "0      0\n",
      "Empty DataFrame\n",
      "Columns: [type, count]\n",
      "Index: []\n",
      "\n",
      "META_KEYWORDS:\n",
      "   count\n",
      "0  29418\n",
      "          type  count\n",
      "0     reliable     12\n",
      "1         fake     22\n",
      "2    clickbait    127\n",
      "3       satire    262\n",
      "4   unreliable    469\n",
      "5      unknown    662\n",
      "6      junksci   3235\n",
      "7         None   4199\n",
      "8         bias   4681\n",
      "9   conspiracy   5876\n",
      "10   political  16542\n",
      "\n",
      "TAGS:\n",
      "   count\n",
      "0  36876\n",
      "          type  count\n",
      "0        rumor     41\n",
      "1         hate     65\n",
      "2     reliable    119\n",
      "3   unreliable    715\n",
      "4         None    761\n",
      "5      junksci    824\n",
      "6       satire    911\n",
      "7      unknown   1078\n",
      "8         fake   3218\n",
      "9         bias   6598\n",
      "10   clickbait   8452\n",
      "11  conspiracy  12355\n",
      "12   political  12607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_counts(name, relation, title):\n",
    "    query1 = '''\n",
    "    SELECT count(distinct ''' + name + ''') FROM ''' + relation + ''';\n",
    "    '''\n",
    "    \n",
    "    query2 = '''\n",
    "    SELECT type, count(distinct ''' + name + ''') count FROM ''' + relation + '''\n",
    "    JOIN articles a ON a.id = ''' + relation + '''.article_id\n",
    "    GROUP BY type\n",
    "    ORDER BY count;\n",
    "    '''\n",
    "    \n",
    "    print(title + ':')\n",
    "    print(psql.read_sql(query1, pg_conn))\n",
    "    print(psql.read_sql(query2, pg_conn))\n",
    "    print()\n",
    "\n",
    "get_counts('keyword_name', 'keywords', \"KEYWORDS\")\n",
    "get_counts('meta_keyword_name', 'meta_keywords', \"META_KEYWORDS\")\n",
    "get_counts('tag_name', 'tags', \"TAGS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A final observation one can make using SQL-queries is the average amount of authors of articles of different types, as answered by the below query.\n",
    "\n",
    "It can be seen that fake articles have the lowest amount of authors, which was also our hypothesis.\n",
    "\n",
    "Further, we can see that reliable articles have a large amount of authors on average, only beaten by hate-articles.\n",
    "\n",
    "The fact that hate-articles have the largest amount of authors is a bit surprising, and would warrant further investigation into whether this is due to artifacts or is actually consistent with reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          type       avg\n",
      "0         fake  1.030210\n",
      "1         None  1.190649\n",
      "2        rumor  1.435897\n",
      "3    clickbait  2.124850\n",
      "4   conspiracy  2.433333\n",
      "5       satire  2.487500\n",
      "6   unreliable  3.270199\n",
      "7      junksci  3.476415\n",
      "8         bias  4.162914\n",
      "9      unknown  4.223118\n",
      "10   political  4.651550\n",
      "11    reliable  5.746835\n",
      "12        hate  7.580769\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT type, avg(count) avg FROM articles\n",
    "JOIN (\n",
    "    SELECT article_id, count(author_name) count FROM authors\n",
    "    GROUP BY article_id\n",
    ") author_counts\n",
    "ON article_id = id\n",
    "GROUP BY type\n",
    "ORDER BY avg;\n",
    "'''\n",
    "\n",
    "print(psql.read_sql(query, pg_conn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
